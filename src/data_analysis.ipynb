{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning \n",
    "\n",
    "Analysing the dataset's information we can see that the shared data are the trip duration (although this figure is not always given explicitly but can be calculated by having the time of departure and arrival), the starting and ending station (name or ID), the user type and the bike ID.\n",
    "\n",
    "These data are not present in all datasets: duration is not present in the newer version of CityBike NYC, in CoGo's trips data and in Divvy's data, start and end times are not present in the Austin MetroBike (it is only present the end time and the duration so we can infer that), start and end stations are not present in the Austin MetroBike and the bike id is not present in Divvy's dataset.\n",
    "It therefore seems impossible to have data shared by all datasets but those reported to you may be the most important ones to work on.\n",
    "\n",
    "Another information that is present in some of the dataset are the year of Birth, the gender and the station latitude and longitude. The first two data can be useful for a social analysis on the age and the gender of the user in different cities but this data are only presented in the older version of the CityBike NYC, in BluBike (through ... check) and in CoGo's dataset (randomly, in some months it is present and in others it is not, and it changes format. These data are present only through January 2020).\n",
    "\n",
    "Starting year of each dataset:\n",
    "- CityBike NYC, 2013\n",
    "- BlueBike Boston, 2015\n",
    "- Capital BikeShare Washington, 2010\n",
    "- Bay Wheels San Francisco, 2017\n",
    "- CoGo Columbus, 2018\n",
    "- Austin MetroBike, 2014 (mixed)\n",
    "- Divvy Chicago, (2014, incomplete) 2020\n",
    "- IndeGo Philly, 2015 (incomplete)\n",
    "\n",
    "Note: 2018-01 Columbus not present, 2018-02 has wrong data\n",
    "\n",
    "These functions are currently usable with the dataset of NYC, Boston, Washington and San Francisco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "data_dir = 'D:\\\\unitn\\\\Bike-Inequality\\\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_data(df):\n",
    "    # Get the number of missing data points per column\n",
    "    missing_values_count = df.isnull().sum()\n",
    "\n",
    "    # How many total missing values do we have?\n",
    "    total_cells = np.prod(df.shape)\n",
    "    total_missing = missing_values_count.sum()\n",
    "\n",
    "    # Percent of data that is missing\n",
    "    percent_missing = (total_missing/total_cells) * 100\n",
    "\n",
    "    return percent_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the duration information\n",
    "The function get_duration_info() takes a DataFrame, analyzes the data internally and clusters them into five different classes: '< 500', '< 1000', '< 10000', '< 100000' and '> 100000'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration_info(df, number, axs, fig, file):\n",
    "    '''\n",
    "    function to get the duration information for each file, cluster them and plot the frequency of each cluster\n",
    "    '''\n",
    "    durations = get_time(file, df, number)\n",
    "\n",
    "    # Define a function to determine the cluster for each number\n",
    "    def get_cluster(num):\n",
    "        if type(num) == str:\n",
    "            num = int(float(num.replace(',', '')))\n",
    "        if num < 500:\n",
    "            return \"<500\"\n",
    "        elif num < 1000:\n",
    "            return \"<1000\"\n",
    "        elif num < 10000:\n",
    "            return \"<10000\"\n",
    "        elif num < 100000:\n",
    "            return \"<100000\"\n",
    "        else:\n",
    "            return \">100000\"\n",
    "\n",
    "    # Group numbers into clusters\n",
    "    clusters = [get_cluster(num) for num in durations]\n",
    "\n",
    "    # Calculate frequency of each cluster\n",
    "    frequency = Counter(clusters)\n",
    "\n",
    "    # Extract cluster names and their frequencies\n",
    "    for key in frequency:\n",
    "        frequency[key] /= sum(frequency.values())\n",
    "    \n",
    "    return frequency\n",
    "\n",
    "def prepare_plot(axs, fig, frequency, number):\n",
    "    x = number // 3\n",
    "    y = number % 3\n",
    "    cluster_names = list(frequency.keys())\n",
    "    cluster_frequencies = list(frequency.values())\n",
    "    axs[x, y].bar(cluster_names, cluster_frequencies)\n",
    "    for index in range(len(cluster_names)):\n",
    "        axs[x, y].text(index, cluster_frequencies[index], str(round(cluster_frequencies[index], 5)), ha='center')\n",
    "\n",
    "    # Add labels and title\n",
    "    for ax in axs.flat:\n",
    "        ax.set(xlabel='Trip duration', ylabel='Frequency')\n",
    "\n",
    "    # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "    for ax in axs.flat:\n",
    "        ax.label_outer()\n",
    "    fig.suptitle('Frequency of Trip Duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "## Monthly duration frequency of ride \n",
    "Data is divided into multiple file so we need to integrate in a single file the data of a single month in order to evaluate them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_for_month(data_dir):\n",
    "\n",
    "    data_files = os.listdir(data_dir)\n",
    "    month = 0\n",
    "\n",
    "    fig, ax = plt.subplots(4, 3)\n",
    "\n",
    "    for file in data_files:\n",
    "        # read data from each file into a DataFrame\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if 'Philly' in data_dir or ('Chicago' in file_path and file_path[18:24] < '202004') or  ('Chicago' in file_path and '2020Q1' in file_path):\n",
    "            month = (int(file[5:6]) - 1) * 3\n",
    "            for _ in range(3):\n",
    "                frequency = get_duration_info(df, month, ax, fig, file_path)\n",
    "                prepare_plot(ax, fig, frequency, month, data_dir)\n",
    "                month += 1\n",
    "        else:\n",
    "            month = int(file[4:6]) - 1\n",
    "            frequency = get_duration_info(df, month, ax, fig, file_path)\n",
    "            prepare_plot(ax, fig, frequency, month, data_dir)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yearly duration frequency\n",
    "The function data_for_year() take the data from each file and return the frequency for each ride duration interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_for_year(data_dir):\n",
    "    '''\n",
    "    function to plot the data for each year\n",
    "    '''\n",
    "\n",
    "    fig, ax = plt.subplots(2, 3)\n",
    "    yearly_frequency = {\n",
    "        '<500': 0,\n",
    "        '<1000': 0,\n",
    "        '<10000': 0,\n",
    "        '<100000': 0,\n",
    "        '>100000': 0\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for year in os.listdir(data_dir):\n",
    "        year_path = os.path.join(data_dir, year)\n",
    "        for file in os.listdir(year_path):\n",
    "            file_path = os.path.join(year_path, file)\n",
    "            df = pd.read_csv(file_path, dtype='object')\n",
    "            frequency = get_duration_info(df, year, ax, fig, file_path)\n",
    "            for key in frequency:\n",
    "                yearly_frequency[key] += frequency[key]\n",
    "        year = int(year) - 2018\n",
    "        total = sum(yearly_frequency.values())\n",
    "        for key in yearly_frequency:\n",
    "            yearly_frequency[key] /= total\n",
    "        prepare_plot(ax, fig, yearly_frequency, year, year_path)\n",
    "        frequency.clear()\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender analysis\n",
    "The function data_integration_for_gender() take the data from each file and count how many people for each gender there are in it and add it to the dictionary genders_infos.\n",
    "At the end plot the data with a pie chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_for_gender(data_dir):\n",
    "\n",
    "    data_files = os.listdir(data_dir)\n",
    "    genders_infos = {\n",
    "        'unknown': 0,\n",
    "        'men': 0,\n",
    "        'women': 0,\n",
    "    }\n",
    "\n",
    "    for year in data_files:\n",
    "        # read data from each file into a DataFrame\n",
    "        year_path = os.path.join(data_dir, year)\n",
    "        for file in os.listdir(year_path):\n",
    "            \n",
    "            file_path = os.path.join(year_path, file)\n",
    "            \n",
    "            df = pd.read_csv(file_path, dtype='object')\n",
    "            count = df['gender'].value_counts()\n",
    "            # plot the percentage of each data\n",
    "            if 'unknown' in count:\n",
    "                genders_infos['unknown'] += count.loc['unknown']\n",
    "            if '1' in count:\n",
    "                genders_infos['unknown'] += count.loc['0']\n",
    "                genders_infos['men'] += count.loc['1']\n",
    "                genders_infos['women'] += count.loc['2']\n",
    "            if 'Male' in count:\n",
    "                if 'Unknown' in count:\n",
    "                    genders_infos['unknown'] += count.loc['Unknown']\n",
    "                genders_infos['men'] += count.loc['Male']\n",
    "                genders_infos['women'] += count.loc['Female']\n",
    "    \n",
    "    print(genders_infos)\n",
    "    if genders_infos['men'] != 0 or genders_infos['women'] != 0:\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.title(data_dir)\n",
    "        plt.pie(genders_infos.values(), labels=genders_infos.keys(),  autopct='%1.1f%%')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('Gender data not available in this city:', data_dir[5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Type analysis\n",
    "The function data_integration_for_usertype() take the data from each file and count how many people for each subscription there are in it and add it to the dictionary usertypes_infos.\n",
    "At the end plot the data with a pie chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_for_usertype(data_dir):\n",
    "    \n",
    "    data_files = os.listdir(data_dir)\n",
    "    if 'Philly' in data_dir:\n",
    "        usertypes_infos = {\n",
    "            'Indego30': 0,\n",
    "            'Indego365': 0,\n",
    "            'Walk-up': 0,\n",
    "            'Day Pass': 0,\n",
    "            'IndegoFlex': 0,\n",
    "        }\n",
    "    elif 'Austin' in data_dir:\n",
    "        usertypes_infos = {\n",
    "            'Walk Up': 0,\n",
    "            'Local365': 0,\n",
    "            'Local30': 0,\n",
    "            'Local365+Guest Pass': 0,\n",
    "            'Republic Rider': 0,\n",
    "        }\n",
    "    else:\n",
    "        usertypes_infos = {\n",
    "            'unknown': 0,\n",
    "            'customers': 0,\n",
    "            'subscribers': 0,\n",
    "        }\n",
    "\n",
    "    for year in data_files:\n",
    "        # read data from each file into a DataFrame\n",
    "        year_path = os.path.join(data_dir, year)\n",
    "        \n",
    "        for file in os.listdir(year_path):\n",
    "            file_path = os.path.join(year_path, file)\n",
    "        \n",
    "            df = pd.read_csv(file_path, dtype='object')\n",
    "            count = df['usertype'].value_counts()\n",
    "            if 'unknown' in count:\n",
    "                usertypes_infos['unknown'] += count['unknown']\n",
    "            if 'Customer' in count:\n",
    "                usertypes_infos['customers'] += count['Customer']\n",
    "                usertypes_infos['subscribers'] += count['Subscriber']\n",
    "            if 'member' in count:\n",
    "                usertypes_infos['customers'] += count['casual']\n",
    "                usertypes_infos['subscribers'] += count['member']\n",
    "            if 'Member' in count:\n",
    "                usertypes_infos['customers'] += count['Casual']\n",
    "                usertypes_infos['subscribers'] += count['Member']\n",
    "            if 'Philly' in file_path:\n",
    "                if 'Indego30' in count:\n",
    "                    usertypes_infos['Indego30'] += count['Indego30']\n",
    "                if 'Indego365' in count:\n",
    "                    usertypes_infos['Indego365'] += count['Indego365']\n",
    "                if 'Walk-up' in count:\n",
    "                    usertypes_infos['Walk-up'] += count['Walk-up']\n",
    "                if 'Day Pass' in count:\n",
    "                    usertypes_infos['Day Pass'] += count['Day Pass']\n",
    "                if 'IndegoFlex' in count:\n",
    "                    usertypes_infos['IndegoFlex'] += count['IndegoFlex']\n",
    "            if 'Austin' in file_path:\n",
    "                if 'Walk Up' in count:\n",
    "                    usertypes_infos['Walk Up'] += count['Walk Up']\n",
    "                if 'Local365' in count:\n",
    "                    usertypes_infos['Local365'] += count['Local365']\n",
    "                if 'Local30' in count:\n",
    "                    usertypes_infos['Local30'] += count['Local30']\n",
    "                if 'Local365+Guest Pass' in count:\n",
    "                    usertypes_infos['Local365+Guest Pass'] += count['Local365+Guest Pass']\n",
    "                if 'Republic Rider' in count:\n",
    "                    usertypes_infos['Republic Rider'] += count['Republic Rider (Annual)']\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title(data_dir)\n",
    "    print(usertypes_infos)\n",
    "    plt.pie(usertypes_infos.values(), labels=usertypes_infos.keys(),  autopct='%1.1f%%')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City: Austin\n",
      "{'unknown': 1407583, 'men': 0, 'women': 0}\n",
      "Gender data not available in this city: tn\\Bike-Inequality\\data\\Austin\n",
      "City: Boston\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 12\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" for year in os.listdir(data_dir_city):\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m    print('Year:', year)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    data_dir_city_year = os.path.join(data_dir_city, year)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    data_for_month(data_dir_city_year) \"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#data_for_year(data_dir_city)\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mdata_for_gender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir_city\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[93], line 17\u001b[0m, in \u001b[0;36mdata_for_gender\u001b[1;34m(data_dir)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(year_path):\n\u001b[0;32m     15\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(year_path, file)\n\u001b[1;32m---> 17\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     count \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# plot the percentage of each data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gaiap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gaiap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gaiap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\gaiap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen codecs>:331\u001b[0m, in \u001b[0;36mgetstate\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for file in os.listdir(data_dir):\n",
    "    print('City:', file)\n",
    "    data_dir_city = os.path.join(data_dir, file)\n",
    "    \n",
    "    # plot date for each month\n",
    "    \"\"\" for year in os.listdir(data_dir_city):\n",
    "        print('Year:', year)\n",
    "        data_dir_city_year = os.path.join(data_dir_city, year)\n",
    "        data_for_month(data_dir_city_year) \"\"\"\n",
    "    \n",
    "    #data_for_year(data_dir_city)\n",
    "    data_for_gender(data_dir_city)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
